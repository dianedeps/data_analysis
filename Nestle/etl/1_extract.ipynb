{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f1d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26b3ffbc",
   "metadata": {},
   "source": [
    "Setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cbab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_wd   = os.getcwd() \n",
    "path_root = path_wd.rsplit(\"\\\\\",1)[0]\n",
    "\n",
    "path_data = path_root + '\\\\data\\\\'\n",
    "path_etl  = path_root + '\\\\etl\\\\'\n",
    "\n",
    "path_user = os.path.expanduser('~')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cb1923e",
   "metadata": {},
   "source": [
    "Data Extraction and Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce4cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a data frame with [\"full_path\", \"item\", \"path\", \"file\", (\"extension\")] \n",
    "# root_path -> file path in windows format\n",
    "# extension = [1, 0] -> extension of the file based on the name\n",
    "def list_files(root_path, extension):\n",
    "    if extension == 1:\n",
    "        cols = [\"full_path\", \"item\", \"path\", \"file\", \"extension\"]\n",
    "    else:\n",
    "        cols = [\"full_path\", \"item\", \"path\", \"file\"]\n",
    "    \n",
    "    df_final = pd.DataFrame(columns = cols)\n",
    "    df = pd.DataFrame(columns = cols)\n",
    "\n",
    "    # List with all items into 'root_path'\n",
    "    list_root_path = os.listdir(root_path)\n",
    "    \n",
    "    # Iterates through root_path items list and concat info: [cols] to final df\n",
    "    for i in range(len(list_root_path)):\n",
    "        item = list_root_path[i]\n",
    "        full_path = root_path + item\n",
    "\n",
    "        if os.path.isdir(full_path):\n",
    "            path_list = os.listdir(full_path)\n",
    "\n",
    "            # Iterates through directory items list \n",
    "            for j in range(len(path_list)):\n",
    "                file = path_list[j]\n",
    "                f_type = file.rsplit(\".\", 1)[1] # maxsplit parameter = 1 returns 2 values, then select list position value, same logic\n",
    "\n",
    "                if extension == 1:\n",
    "                    data = [full_path+'\\\\'+file, item, full_path, file, f_type]\n",
    "                else:\n",
    "                    data = [full_path+'\\\\'+file, item, full_path, file]\n",
    "\n",
    "                df.loc[j] = data\n",
    "\n",
    "        elif os.path.isfile(full_path):\n",
    "            file = item\n",
    "            f_type = file.rsplit(\".\", 1)[1]\n",
    "\n",
    "            if extension == 1:\n",
    "                df = pd.DataFrame([[full_path, full_path.rsplit(\"\\\\\")[-2], root_path, file, f_type]], columns = cols)\n",
    "            else:\n",
    "                df = pd.DataFrame([[full_path, full_path.rsplit(\"\\\\\")[-2], root_path, file]], columns = cols)\n",
    "\n",
    "        else:\n",
    "            file = item\n",
    "            f_type = file.rsplit(\".\", 1)[1]\n",
    "\n",
    "            if extension == 1:\n",
    "                df = pd.DataFrame([[full_path, full_path.rsplit(\"\\\\\")[-1], root_path, file, f_type]], columns = cols)\n",
    "            else:\n",
    "                df = pd.DataFrame([[full_path, full_path.rsplit(\"\\\\\")[-1], root_path, file]], columns = cols)\n",
    "\n",
    "        df_final = pd.concat([df_final, df], ignore_index = True)\n",
    "        df = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524cd3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function clean csv file separators\n",
    "def clean_csv_separator(input_file):\n",
    "    try:\n",
    "        output_file = path_etl + \"cleansing\\\\\" + input_file.rsplit(\"\\\\\")[-1].replace('.csv', '_formatted.csv')\n",
    "\n",
    "        with open(input_file, 'r') as f_in, open(output_file, 'w', newline='') as f_out:\n",
    "            rows = f_in.readlines()\n",
    "            format_rows = []\n",
    "            separators = [';;', '||', '|', '%', ';']\n",
    "\n",
    "            for r in rows:\n",
    "                for s in separators:\n",
    "                    r = r.replace(s, ',')\n",
    "                format_rows.append(r)\n",
    "\n",
    "            f_out.writelines(format_rows)\n",
    "\n",
    "        print(f\"New file created: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a117f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in data directory\n",
    "df_raw_files_list = list_files(path_data, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dd12a1c",
   "metadata": {},
   "source": [
    "CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b34edaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file created: c:\\Users\\Diane\\git\\data_analysis\\Nestle\\etl\\cleansing\\BaseCargos_formatted.csv\n",
      "New file created: c:\\Users\\Diane\\git\\data_analysis\\Nestle\\etl\\cleansing\\BaseCEP_formatted.csv\n",
      "New file created: c:\\Users\\Diane\\git\\data_analysis\\Nestle\\etl\\cleansing\\BaseClientes_formatted.csv\n",
      "New file created: c:\\Users\\Diane\\git\\data_analysis\\Nestle\\etl\\cleansing\\BaseFuncionarios_formatted.csv\n",
      "New file created: c:\\Users\\Diane\\git\\data_analysis\\Nestle\\etl\\cleansing\\BaseNivel_formatted.csv\n",
      "New file created: c:\\Users\\Diane\\git\\data_analysis\\Nestle\\etl\\cleansing\\BasePQ_formatted.csv\n"
     ]
    }
   ],
   "source": [
    "df_csv_list = df_raw_files_list[df_raw_files_list['extension'] == 'csv']\n",
    "\n",
    "# iterate through the list and clean files\n",
    "for i in range(len(df_csv_list)):\n",
    "    f_name = df_csv_list['full_path'].iloc[i]\n",
    "\n",
    "    df_clean = clean_csv_separator(f_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0e18826",
   "metadata": {},
   "source": [
    "JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fa3b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function clean json file bugs\n",
    "def clean_json_file(input_file):\n",
    "    try:\n",
    "        output_file = path_etl + \"cleansing\\\\\" + input_file.rsplit(\"\\\\\")[-1].replace('.json', '_formatted.json')\n",
    "\n",
    "        with open(input_file, 'r') as f_in, open(output_file, 'w', newline='') as f_out:\n",
    "            rows = f_in.readlines()\n",
    "            format_rows = []\n",
    "\n",
    "            separators = [\"\\\";\"]\n",
    "            start_lines = ['\\\"{']\n",
    "            end_lines = ['}\\\"']\n",
    "            q_marks = ['\"\"\"', '\"\"', '\\'']\n",
    "\n",
    "            for r in rows:\n",
    "                for s in separators:\n",
    "                    r = r.replace(s, ',')\n",
    "                for t in start_lines:\n",
    "                    r = r.replace(t, '{')\n",
    "                for e in end_lines:\n",
    "                    r = r.replace(e, '},')\n",
    "                for q in q_marks:\n",
    "                    r = r.replace(q, '\"')\n",
    "                format_rows.append(r)\n",
    "            \n",
    "            format_rows[0] = \"[\\n\"\n",
    "            format_rows[-1] = format_rows[-1].rstrip() + ']\\n'\n",
    "            format_rows[-1] = format_rows[-1].replace('},]', '}\\n]')\n",
    "\n",
    "            formatted_json = json.dumps(json.loads(''.join(format_rows)), indent=4)\n",
    "            f_out.write(formatted_json)\n",
    "\n",
    "        print(f\"New file created: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e7b3a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file created: c:\\Users\\Diane\\git\\data_analysis\\Nestle\\etl\\cleansing\\Vendas_formatted.json\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_json_list = df_raw_files_list[df_raw_files_list['extension'] == 'json']\n",
    "\n",
    "# iterate through the list and clean files\n",
    "for i in range(len(df_json_list)):\n",
    "    f_name = df_json_list['full_path'].iloc[i]\n",
    "\n",
    "    df_clean = clean_json_file(f_name)\n",
    "\n",
    "print(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb4efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is valid.\n"
     ]
    }
   ],
   "source": [
    "def validate_json_file(f_name):\n",
    "    try:\n",
    "        with open(f_name, 'r') as f:\n",
    "            json.load(f)\n",
    "        print(\"File is valid.\")\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"File is invalid. Error message:\")\n",
    "        print(e)\n",
    "\n",
    "f_name = r'c:\\Users\\Diane\\git\\data_analysis\\Nestle\\etl\\cleansing\\Vendas_formatted.json'\n",
    "validate_json_file(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7ab2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diane\\git\\data_analysis\\Nestle\\etl\\list_etl_files.csv\n"
     ]
    }
   ],
   "source": [
    "# List all files in etl directory\n",
    "df_etl_files_list = list_files(path_etl, 1)\n",
    "\n",
    "out_csv = path_etl+'list_etl_files.csv'\n",
    "df_etl_files_list.to_csv(out_csv, index=False)\n",
    "print(out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a71f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_json_to_csv(json_file, out_csv):\n",
    "    df = pd.json_normalize(json_file, sep='.')\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f'JSON data was exported to \"{out_csv}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab286e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data was exported to \"c:\\Users\\Diane\\git\\data_analysis\\Nestle\\etl\\cleansing\\Vendas_formatted_csv.csv\"\n"
     ]
    }
   ],
   "source": [
    "df_json_list = df_etl_files_list[df_etl_files_list['extension'] == 'json']\n",
    "\n",
    "# iterate through the list and export to csv files\n",
    "for i in range(len(df_json_list)):\n",
    "    in_json = df_json_list['full_path'].iloc[i]\n",
    "    out_csv = path_etl + \"cleansing\\\\\" + in_json.rsplit(\"\\\\\")[-1].replace('.json', '_csv.csv')\n",
    "\n",
    "    with open(in_json, 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "\n",
    "    export_json_to_csv(json_file, out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf9ff739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a data frame object based on listed file\n",
    "def get_raw_file(full_path):\n",
    "    if full_path.split('.')[1] == 'csv':\n",
    "        df_raw = pd.read_csv(full_path, delimiter=\",\")\n",
    "    elif full_path.split('.')[1] == 'xlsx':\n",
    "        df_raw = pd.read_excel(full_path)\n",
    "    elif full_path.split('.')[1] == 'json':\n",
    "        df_raw = pd.read_json(full_path)\n",
    "\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dd13e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in etl directory\n",
    "df_etl_files_list = list_files(path_etl, 1)\n",
    "df_list = df_etl_files_list[df_etl_files_list['extension'] == 'csv']\n",
    "# display(df_list)\n",
    "\n",
    "df__list_vendas = df_list.loc[df_list['file'].str.contains('Vendas')].values[0]\n",
    "f_name = df__list_vendas[0]\n",
    "\n",
    "df_vendas = get_raw_file(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e84191fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select head df\n",
    "df_head = df_vendas.columns.tolist()\n",
    "\n",
    "# select columns with client's info\n",
    "df_head_client = df_head[:15]\n",
    "\n",
    "i = 1\n",
    "while i <= 12:\n",
    "    # select columns that contains 'meses'\n",
    "    df_head_mes = [j for j in df_head if ('meses.'+str(i))+'.' in j]\n",
    "    \n",
    "    df_head_to_filter = df_head_client + df_head_mes\n",
    "\n",
    "    df_meses = df_vendas[df_head_to_filter]\n",
    "    \n",
    "    out_csv = path_etl + 'cleansing\\\\' + 'vendas_mes_' + str(i) + '_formatted.csv'\n",
    "    df_meses.to_csv(out_csv, index=False)\n",
    "\n",
    "    i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c8a37b3b3643c9fb15903c055339ac833392cb866e775b4277fbf96facdfc8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
